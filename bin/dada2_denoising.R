library(logger)
log_threshold(WARN)

load_library <- function(library_name) {
  output <- capture.output({
    suppressWarnings({
      library(library_name, character.only = TRUE)
    })
  }, type = "message")
  
  # Separate warnings from messages
  warnings <- warnings()
  
  # Log messages
  if(length(output) > 0) {
    log_info(paste("Message from", library_name, ":", paste(output, collapse = "; ")))
  }
  
  # Log warnings
  if(length(warnings) > 0) {
    log_warn(paste("Warning from", library_name, ":", paste(warnings, collapse = "; ")))
  }
}

load_library("lobstr")
# Define profiling function
profile_function <- function(func, ...) {
  # Record the memory size of the arguments
  mem_args <- lobstr::obj_size(...)

  # Variable to store messages
  messages_captured <- character(0)

  # Record start time and memory
  time_start <- proc.time()[["elapsed"]]
  mem_start <- lobstr::mem_used()

  # Evaluate the function while capturing its output and messages
  output_captured <- capture.output({
      withCallingHandlers({
          result <- func(...)
      }, message = function(m) {
          # Capture just the content of the message
          messages_captured <<- c(messages_captured, conditionMessage(m))
      })
  })

  # Record end time and memory
  time_end <- proc.time()[["elapsed"]]
  mem_end <- lobstr::mem_used()

  # Combine standard output and messages
  all_captured <- c(output_captured, messages_captured)

  # Return results including captured output and messages
  list(
      result = result, 
      output = all_captured,
      mem_args = mem_args,
      mem_diff = (mem_end - mem_start) - mem_args,
      duration = time_end - time_start
  )
}


load_library("dada2")
load_library("tidyverse")
load_library("argparse")

parser <- ArgumentParser(description='DADA2 Denoising of Amplicon Sequences from a Single Sample')
parser$add_argument('--derep-1', type="character", required=TRUE, nargs="+", help="Paths to RDS files containing dereplicated forward sequences")
parser$add_argument('--derep-2', type="character", required=TRUE, nargs="+", help="Paths to RDS files containing dereplicated reverse sequences")
parser$add_argument('--error-model-1', type="character", required=TRUE, help="Path to the forward error model RDS file")
parser$add_argument('--error-model-2', type="character", required=TRUE, help="Path to the reverse error model RDS file")
parser$add_argument('--ncores', type="numeric", required=TRUE, help="Number of threads to use")
parser$add_argument('--pool', type="character", default="false")
parser$add_argument('--band-size', type='integer', default=16)
parser$add_argument('--omega-a', type='double', default=1e-120)
parser$add_argument('--just-concatenate', action='store_true')
parser$add_argument('--use-quals', type="character", default="false")
parser$add_argument('--maxEE', type="integer", default=2)
parser$add_argument('--self-consist', action='store_true')
parser$add_argument('--omega-c', type='double', default=1e-40)
parser$add_argument('--ampliconFILE', type="character", required=TRUE, help="Path to amplicon info file")
parser$add_argument('--dout', type="character", required=FALSE, help="Output directory")
parser$add_argument('--log-level', type="character", default = "INFO", help = "Log level. Default is INFO.")

args <- parser$parse_args()
# Set up logging
log_level_arg <- match.arg(args$log_level, c("DEBUG", "INFO", "WARN", "ERROR", "FATAL"))
log_threshold(log_level_arg)

args_string <- paste(sapply(names(args), function(name) {
  paste(name, ":", args[[name]])
}), collapse = ", ")

log_debug(paste("Arguments parsed successfully:", args_string))

# Filter samples based on the provided condition
filter_samples <- function(samples, condition) {
  filtered_samples <- samples[names(samples) %in% rownames(condition)]
  log_debug(paste("Filtered samples:", toString(names(filtered_samples))))
  return(filtered_samples)
}

get_sample_name <- function(filename, loci_list) {
  # Remove locus from the beginning
  no_locus_filename <- gsub(paste0("^(", paste(loci_list, collapse="|"), ")_"), "", filename)
  
  # Remove everything past '_trimmed'
  sample_name <- strsplit(no_locus_filename, "_trimmed")[[1]][1]

  log_debug(paste("Extracted sample name:", sample_name, "from filename:", filename))
  
  return(sample_name)
}

merge_with_concatenation <- function(dadaFs, derepFs, dadaRs, derepRs, amplicon_info) {
  log_debug("Performing sample filtering based on overlap conditions")
  # Perform filtering
  overlap_condition <- amplicon_info %>% filter((ampInsert_length + 10) < sum.mean.length.reads)
  log_debug(paste("Number of samples with overlap condition:", nrow(overlap_condition)))

  no_overlap_condition <- amplicon_info %>% filter((ampInsert_length + 10) >= sum.mean.length.reads)
  log_debug(paste("Number of samples with no overlap condition:", nrow(no_overlap_condition)))
  
  log_debug("Merging samples with overlap condition")
  # Merge for overlap and no overlap conditions
  mergers.overlap <- perform_merging(filter_samples(dadaFs, overlap_condition),
                                     filter_samples(derepFs, overlap_condition),
                                     filter_samples(dadaRs, overlap_condition),
                                     filter_samples(derepRs, overlap_condition),
                                     justConcatenate=FALSE)

  log_debug("Merging samples with no overlap condition")
  mergers.no.overlap <- perform_merging(filter_samples(dadaFs, no_overlap_condition),
                                        filter_samples(derepFs, no_overlap_condition),
                                        filter_samples(dadaRs, no_overlap_condition),
                                        filter_samples(derepRs, no_overlap_condition),
                                        justConcatenate=TRUE)

  # Combine the results
  combined_results <- c(mergers.overlap, mergers.no.overlap)[names(dadaFs)]
  log_debug(paste("Total merged samples:", length(combined_results)))

  return(combined_results)
}

perform_merging <- function(dadaFs_selected, derepFs_selected, dadaRs_selected, derepRs_selected, justConcatenate) {
  log_debug("Performing merging")
  merged_pairs <- mergePairs(dadaFs_selected, derepFs_selected, dadaRs_selected, derepRs_selected, 
             verbose = TRUE, 
             justConcatenate = justConcatenate, 
             trimOverhang = TRUE,
             minOverlap = 10,
             maxMismatch = 1,
             propagateCol = c("sequence", "quality", "n0"))
  log_debug(paste("Number of merged pairs:", length(merged_pairs)))
  log_debug("Merging complete")
  return(merged_pairs)
}

#############
### Main ####
#############

log_info(paste("Loading R1 error model from:", args$error_model_1))
log_info(paste("Loading R2 error model from:", args$error_model_2))

# Load error models
err_model_F <- readRDS(args$error_model_1)
err_model_R <- readRDS(args$error_model_2)

# Function to gather derep objects from a list of files
gather_dereps <- function(paths) {
    all_dereps <- list()
    for(path in paths) {
        derep <- readRDS(path)
        # Extract the sample name using a regex pattern
        sample_name <- sub("(.*)_trimmed.*", "\\1", basename(path))
        all_dereps[[sample_name]] <- derep
    }
    all_dereps
}

log_info("Gathering derep objects")
# Gather all derep objects
derepFs <- gather_dereps(args$derep_1)
derepRs <- gather_dereps(args$derep_2)

# DADA2 denoising
log_info("Starting DADA2 denoising")
# Profile DADA2 denoising
profiling_denoising <- profile_function(function(...) {
    log_info("Denoising forward reads")
    dadaFs <- dada(derepFs, err=err_model_F, multithread=args$ncores)
    log_info("Denoising reverse reads")
    dadaRs <- dada(derepRs, err=err_model_R, multithread=args$ncores)
    list(dadaFs, dadaRs)
})
dadaFs <- profiling_denoising$result[[1]]
dadaRs <- profiling_denoising$result[[2]]
log_info(paste("Time taken for DADA2 denoising:", profiling_denoising$duration, "seconds"))
log_info(paste("Memory change during DADA2 denoising:", profiling_denoising$mem_diff, "bytes"))
# Log the captured output
if(length(profiling_denoising$output) > 0) {
    log_info("Output from dada:\n{paste(profiling_denoising$output, collapse = '\n')}")
}


# Save the DADA2 denoised data
log_info("Saving DADA2 denoised data")
save(dadaFs, file="dada_denoised_F.RData")
save(dadaRs, file="dada_denoised_R.RData")

log_info("Reading amplicon data")
amplicon_data <- read.table(args$ampliconFILE, header=T)
loci_list <- amplicon_data$amplicon

sample_name <- get_sample_name(names(derepFs)[1], loci_list)  # extract sample name
log_debug(paste("Extracted sample name:", sample_name))

# Setup output directory (if asked)
output_dir <- if (is.null(args$dout)) {
  "."
} else {
  args$dout
}

log_info(paste("Output directory:", output_dir))

# If the output directory does not exit, create it
if (!dir.exists(output_dir)) {
  log_info(paste("Creating output directory:", output_dir))
  dir.create(output_dir, recursive=TRUE)
}

if (args$just_concatenate) {
  log_info("Performing merging with concatenation")
  amplicon.info = data.frame(
    names = sapply(strsplit(names(dadaFs),"_trimmed"),"[",1)
  )

  log_debug(paste("Initial number of names in amplicon.info:", nrow(amplicon.info)))

  # Create amplicon info
  amplicon.info <- amplicon.info %>%
    mutate(names=sapply(str_split(names,'_S(\\d+)'),head,1)) %>% 
    mutate(amplicon=unlist(lapply(str_split(names,'_'), function(x) { paste(x[1:3], collapse = "_") }))) %>%
    inner_join(amplicon_data %>%
               select(amplicon, ampInsert_length), by = c("amplicon")) %>%
    select(amplicon, ampInsert_length) %>%
    mutate(
      sum.mean.length.reads = sapply(sapply(unlist(lapply(dadaFs,"[","sequence"),recursive = F),nchar),mean)+
        sapply(sapply(unlist(lapply(dadaRs,"[","sequence"),recursive = F),nchar),mean)
    )

  log_debug(paste("Number of amplicons after inner join:", nrow(amplicon.info)))

  rownames(amplicon.info) = names(dadaFs)
  log_debug("Assigned rownames to amplicon.info based on names from dadaFs")

  # Profile the merge_with_concatenation function
  profiling_merge <- profile_function(merge_with_concatenation, dadaFs, derepFs, dadaRs, derepRs, amplicon.info)
  mergers <- profiling_merge$result
  log_info("Time taken for merging with concatenation: {profiling_merge$duration}")

  # Log the captured output
  if(length(profiling_merge$output) > 0) {
    log_info("Output from merge_with_concatenation:\n{paste(profiling_merge$output, collapse = '\n')}")
  }

} else {
   # Profile the perform_merging function
  profiling_merge <- profile_function(perform_merging, dadaFs, derepFs, dadaRs, derepRs, justConcatenate=FALSE)
  mergers <- profiling_merge$result
  log_info("Time taken for normal merging: {profiling_merge$duration}")

  # Log the captured output
  if(length(profiling_merge$output) > 0) {
    log_info("Output from perform_merging:\n{paste(profiling_merge$output, collapse = '\n')}")
  }
}

log_info("Saving merged data and sequence table")
saveRDS(mergers, file = file.path(output_dir, paste0(sample_name, "_merged.RDS")))
log_info(paste("Number of entries in mergers:", length(mergers)))

# Count the number of NULL mergers before filtering
num_null_mergers <- sum(sapply(mergers, is.null))

mergers <- mergers[!sapply(mergers, is.null)]

# If there were any NULL mergers, log a warning
if (num_null_mergers > 0) {
  log_warn(paste(num_null_mergers, "mergers were filtered out due to being NULL."))
}

seqtab <- makeSequenceTable(mergers)
if (ncol(seqtab) == 0) {
  log_warn("No reads were able to merge.")
}

saveRDS(seqtab, file = file.path(output_dir, paste0(sample_name, "_seqtab.RDS")))
log_info("Processing completed")
